{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install xgboost"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UsZc4pr7cN4L",
        "outputId": "683d90d5-62d1-4fb0-8440-1dc8d2239e9f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkj_BIMFcXSP",
        "outputId": "877b5593-1b2d-413a-ee4d-ec156efa1702"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings, json, ast\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier, StackingClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "    print(\"xgboost not installed. Run: pip install xgboost\")\n",
        "\n",
        "def load_and_preprocess(train_path, val_path, test_path):\n",
        "    \"\"\"Load train/val/test datasets and perform all preprocessing steps.\"\"\"\n",
        "    train = pd.read_csv(train_path)\n",
        "    val   = pd.read_csv(val_path)\n",
        "    test  = pd.read_csv(test_path)\n",
        "    train = pd.concat([train, val], ignore_index=True)\n",
        "    print(\"Loaded datasets — Train+Val:\", train.shape, \" Test:\", test.shape)\n",
        "\n",
        "    if \"release_date\" in train.columns:\n",
        "        train[\"release_date\"] = pd.to_datetime(train[\"release_date\"], errors=\"coerce\")\n",
        "        test[\"release_date\"]  = pd.to_datetime(test[\"release_date\"], errors=\"coerce\")\n",
        "        train[\"year\"]  = train[\"release_date\"].dt.year\n",
        "        train[\"month\"] = train[\"release_date\"].dt.month\n",
        "        test[\"year\"]   = test[\"release_date\"].dt.year\n",
        "        test[\"month\"]  = test[\"release_date\"].dt.month\n",
        "\n",
        "    for col in [\"year\", \"month\"]:\n",
        "        train[col] = pd.to_numeric(train.get(col, 2015), errors=\"coerce\").fillna(2015)\n",
        "        test[col]  = pd.to_numeric(test.get(col, 2016), errors=\"coerce\").fillna(2016)\n",
        "    def parse_primary_genre(genres_cell):\n",
        "        if pd.isna(genres_cell): return \"Unknown\"\n",
        "        try:\n",
        "            if isinstance(genres_cell, str):\n",
        "                try:\n",
        "                    val = json.loads(genres_cell)\n",
        "                except Exception:\n",
        "                    val = ast.literal_eval(genres_cell)\n",
        "            else:\n",
        "                val = genres_cell\n",
        "            if isinstance(val, list) and len(val) > 0:\n",
        "                first = val[0]\n",
        "                if isinstance(first, dict) and \"name\" in first:\n",
        "                    return first[\"name\"]\n",
        "                if isinstance(first, str):\n",
        "                    return first\n",
        "            if isinstance(val, dict) and \"name\" in val:\n",
        "                return val[\"name\"]\n",
        "            return str(val)\n",
        "        except Exception:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    if \"primary_genre\" not in train.columns:\n",
        "        if \"genres\" in train.columns:\n",
        "            train[\"primary_genre\"] = train[\"genres\"].apply(parse_primary_genre)\n",
        "            test[\"primary_genre\"]  = test[\"genres\"].apply(parse_primary_genre) if \"genres\" in test.columns else \"Unknown\"\n",
        "        else:\n",
        "            train[\"primary_genre\"] = \"Unknown\"\n",
        "            test[\"primary_genre\"]  = \"Unknown\"\n",
        "\n",
        "    if \"label_roi3\" not in train.columns:\n",
        "        print(\"Creating label_roi3 from budget & revenue.\")\n",
        "        for df in [train, test]:\n",
        "            df[\"budget\"] = pd.to_numeric(df[\"budget\"], errors=\"coerce\")\n",
        "            df[\"revenue\"] = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
        "            df[\"ROI\"] = (df[\"revenue\"] - df[\"budget\"]) / df[\"budget\"]\n",
        "            df[\"ROI\"] = df[\"ROI\"].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "            def roi_to_class(roi):\n",
        "                if pd.isna(roi): return np.nan\n",
        "                if roi >= 2.5567: return \"Hit\"\n",
        "                if roi < 0.0049: return \"Flop\"\n",
        "                return \"Average\"\n",
        "            df[\"label_roi3\"] = df[\"ROI\"].apply(roi_to_class)\n",
        "\n",
        "    train = train.dropna(subset=[\"label_roi3\"])\n",
        "    test  = test.dropna(subset=[\"label_roi3\"])\n",
        "\n",
        "    for col in [\"budget\",\"revenue\",\"year\",\"month\"]:\n",
        "        train[col] = pd.to_numeric(train[col], errors=\"coerce\")\n",
        "        test[col]  = pd.to_numeric(test[col], errors=\"coerce\")\n",
        "\n",
        "    print(\"Finished preprocessing. Label distribution:\")\n",
        "    print(train[\"label_roi3\"].value_counts())\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def train_models(train, test, random_state=42):\n",
        "    \"\"\"Train Tang 2024 Optimized XGBoost and Gupta Ensemble models.\"\"\"\n",
        "    num_features = [\"budget\",\"revenue\",\"year\",\"month\"]\n",
        "    cat_features = [\"primary_genre\"]\n",
        "    target = \"label_roi3\"\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(train[target])\n",
        "    y_test_enc  = le.transform(test[target])\n",
        "\n",
        "    X_train = train[num_features + cat_features].copy()\n",
        "    X_test  = test[num_features + cat_features].copy()\n",
        "\n",
        "    num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "                       (\"oh\", OneHotEncoder(handle_unknown=\"ignore\"))])\n",
        "    pre = ColumnTransformer([(\"num\", num_tf, num_features),\n",
        "                             (\"cat\", cat_tf, cat_features)])\n",
        "\n",
        "    results = {}\n",
        "\n",
        "\n",
        "    print(\"\\n Tang 2024 Optimized XGBoost\")\n",
        "    clf = xgb.XGBClassifier(\n",
        "        objective=\"multi:softprob\",\n",
        "        num_class=len(le.classes_),\n",
        "        eval_metric=\"mlogloss\",\n",
        "        random_state=random_state,\n",
        "        n_estimators=300\n",
        "    )\n",
        "    param_dist = {\n",
        "        \"xgb__max_depth\": [3,4,5,6],\n",
        "        \"xgb__learning_rate\": [0.02,0.05,0.1],\n",
        "        \"xgb__subsample\": [0.8,1.0],\n",
        "        \"xgb__colsample_bytree\": [0.8,1.0],\n",
        "        \"xgb__min_child_weight\": [1,3,5],\n",
        "    }\n",
        "    pipe_xgb = Pipeline([(\"pre\", pre), (\"xgb\", clf)])\n",
        "    search = RandomizedSearchCV(pipe_xgb, param_dist, n_iter=10,\n",
        "                                scoring=\"f1_macro\", cv=3, n_jobs=-1,\n",
        "                                verbose=1, random_state=random_state)\n",
        "    search.fit(X_train, y_train_enc)\n",
        "    best_xgb = search.best_estimator_\n",
        "    pred_xgb_enc = best_xgb.predict(X_test)\n",
        "    pred_xgb = le.inverse_transform(pred_xgb_enc)\n",
        "\n",
        "    results[\"Tang_XGBoost\"] = {\n",
        "        \"Accuracy\": accuracy_score(test[target], pred_xgb),\n",
        "        \"F1_macro\": f1_score(test[target], pred_xgb, average=\"macro\")\n",
        "    }\n",
        "    print(\"Tang XGBoost Results:\", results[\"Tang_XGBoost\"])\n",
        "\n",
        "    print(\"\\n Gupta Ensemble Models\")\n",
        "    lr  = LogisticRegression(max_iter=200)\n",
        "    svm = SVC(probability=True, kernel=\"rbf\", C=2.0, gamma=\"scale\", random_state=random_state)\n",
        "    rf  = RandomForestClassifier(n_estimators=300, random_state=random_state)\n",
        "    gb  = GradientBoostingClassifier(n_estimators=300, learning_rate=0.05, random_state=random_state)\n",
        "    knn = KNeighborsClassifier(n_neighbors=15)\n",
        "    voters = [(\"lr\", lr), (\"svm\", svm), (\"rf\", rf), (\"gb\", gb), (\"knn\", knn)]\n",
        "\n",
        "    soft_vote = VotingClassifier(estimators=voters, voting=\"soft\")\n",
        "    pipe_vote = Pipeline([(\"pre\", pre), (\"ens\", soft_vote)])\n",
        "    pipe_vote.fit(X_train, train[target])\n",
        "    pred_vote = pipe_vote.predict(X_test)\n",
        "    results[\"Voting\"] = {\n",
        "        \"Accuracy\": accuracy_score(test[target], pred_vote),\n",
        "        \"F1_macro\": f1_score(test[target], pred_vote, average=\"macro\")\n",
        "    }\n",
        "    print(\"Voting Results:\", results[\"Voting\"])\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "        estimators=voters,\n",
        "        final_estimator=LogisticRegression(max_iter=200),\n",
        "        stack_method=\"predict_proba\"\n",
        "    )\n",
        "    pipe_stack = Pipeline([(\"pre\", pre), (\"ens\", stack)])\n",
        "    pipe_stack.fit(X_train, train[target])\n",
        "    pred_stack = pipe_stack.predict(X_test)\n",
        "    results[\"Stacking\"] = {\n",
        "        \"Accuracy\": accuracy_score(test[target], pred_stack),\n",
        "        \"F1_macro\": f1_score(test[target], pred_stack, average=\"macro\")\n",
        "    }\n",
        "    print(\"Stacking Results:\", results[\"Stacking\"])\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_results(results_dict):\n",
        "    \"\"\"Print and compare all model results.\"\"\"\n",
        "    print(\"\\n FINAL COMPARISON\")\n",
        "    df = pd.DataFrame(results_dict).T\n",
        "    print(df)\n",
        "    print(\"\\n Best model based on F1_macro:\",\n",
        "          df[\"F1_macro\"].idxmax(),\n",
        "          \"→ F1 =\", round(df[\"F1_macro\"].max(), 4))\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TRAIN_PATH = \"train_movies.csv\"\n",
        "    VAL_PATH   = \"validation_movies.csv\"\n",
        "    TEST_PATH  = \"test_movies.csv\"\n",
        "\n",
        "    train, test = load_and_preprocess(TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
        "    results = train_models(train, test, random_state=42)\n",
        "    summary = evaluate_results(results)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEGEGEl3gqit",
        "outputId": "bef13b49-2c31-46e0-ab72-c65d32736d02"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded datasets — Train+Val: (36373, 24)  Test: (9093, 24)\n",
            "Creating label_roi3 from budget & revenue.\n",
            "Finished preprocessing. Label distribution:\n",
            "label_roi3\n",
            "Flop       4205\n",
            "Average    1681\n",
            "Hit        1320\n",
            "Name: count, dtype: int64\n",
            "\n",
            " Tang 2024 Optimized XGBoost\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Tang XGBoost Results: {'Accuracy': 0.9845605700712589, 'F1_macro': 0.978974825891488}\n",
            "\n",
            " Gupta Ensemble Models\n",
            "Voting Results: {'Accuracy': 0.9946555819477435, 'F1_macro': 0.9935412312633184}\n",
            "Stacking Results: {'Accuracy': 1.0, 'F1_macro': 1.0}\n",
            "\n",
            " FINAL COMPARISON\n",
            "              Accuracy  F1_macro\n",
            "Tang_XGBoost  0.984561  0.978975\n",
            "Voting        0.994656  0.993541\n",
            "Stacking      1.000000  1.000000\n",
            "\n",
            " Best model based on F1_macro: Stacking → F1 = 1.0\n"
          ]
        }
      ]
    }
  ]
}