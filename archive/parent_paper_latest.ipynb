{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHiayoIPuoM9",
        "outputId": "54ef208b-8725-4413-b10a-13384224063e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "catboost not installed. Run: pip install catboost\n",
            "optuna not installed. Run: pip install optuna\n",
            "Loaded datasets — Train+Val: (36373, 24)  Test: (9093, 24)\n",
            "Creating label_roi3 from budget & revenue.\n",
            "Finished preprocessing. Label distribution (train):\n",
            "label_roi3\n",
            "Flop       4205\n",
            "Average    1681\n",
            "Hit        1320\n",
            "Name: count, dtype: int64\n",
            "\n",
            "[Baseline] Tang 2024 Optimized XGBoost (RandomizedSearchCV)\n",
            "Fitting 3 folds for each of 10 candidates, totalling 30 fits\n",
            "Tang XGBoost Results: {'Accuracy': 0.9845605700712589, 'F1_macro': 0.978974825891488}\n",
            "\n",
            "[Baseline] Gupta-style Ensemble Models (Voting & Stacking)\n",
            "Voting Results: {'Accuracy': 0.9946555819477435, 'F1_macro': 0.9935412312633184}\n",
            "Stacking Results: {'Accuracy': 1.0, 'F1_macro': 1.0}\n",
            "\n",
            "[Novelty] LightGBM Model\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000671 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 651\n",
            "[LightGBM] [Info] Number of data points in the train set: 7206, number of used features: 23\n",
            "[LightGBM] [Info] Start training from score -1.455525\n",
            "[LightGBM] [Info] Start training from score -0.538640\n",
            "[LightGBM] [Info] Start training from score -1.697282\n",
            "LightGBM Results: {'Accuracy': 0.9839667458432304, 'F1_macro': 0.9784272959501741}\n",
            "Skip CatBoost (catboost not installed).\n",
            "\n",
            " FINAL COMPARISON \n",
            "              Accuracy  F1_macro\n",
            "Stacking      1.000000  1.000000\n",
            "Voting        0.994656  0.993541\n",
            "Tang_XGBoost  0.984561  0.978975\n",
            "LightGBM      0.983967  0.978427\n",
            "\n",
            "Best model based on F1_macro: Stacking → F1 = 1.0\n"
          ]
        }
      ],
      "source": [
        "import warnings, json, ast\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from sklearn.model_selection import RandomizedSearchCV, cross_val_score\n",
        "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (\n",
        "    classification_report,\n",
        "    accuracy_score,\n",
        "    f1_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        ")\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import (\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    VotingClassifier,\n",
        "    StackingClassifier,\n",
        ")\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "try:\n",
        "    import xgboost as xgb\n",
        "except Exception:\n",
        "    xgb = None\n",
        "    print(\"xgboost not installed. Run: pip install xgboost\")\n",
        "\n",
        "try:\n",
        "    from lightgbm import LGBMClassifier\n",
        "except Exception:\n",
        "    LGBMClassifier = None\n",
        "    print(\"lightgbm not installed. Run: pip install lightgbm\")\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except Exception:\n",
        "    CatBoostClassifier = None\n",
        "    print(\"catboost not installed. Run: pip install catboost\")\n",
        "\n",
        "try:\n",
        "    import optuna\n",
        "except Exception:\n",
        "    optuna = None\n",
        "    print(\"optuna not installed. Run: pip install optuna\")\n",
        "\n",
        "\n",
        "def load_and_preprocess(train_path, val_path, test_path):\n",
        "    \"\"\"\n",
        "    Load train/val/test datasets and perform all preprocessing steps.\n",
        "    This includes:\n",
        "    - Merge train + validation\n",
        "    - Parse release_date to year / month\n",
        "    - Extract primary_genre\n",
        "    - Compute ROI and 3-class label label_roi3: Hit / Average / Flop\n",
        "    \"\"\"\n",
        "    train = pd.read_csv(train_path)\n",
        "    val = pd.read_csv(val_path)\n",
        "    test = pd.read_csv(test_path)\n",
        "\n",
        "    train = pd.concat([train, val], ignore_index=True)\n",
        "    print(\"Loaded datasets — Train+Val:\", train.shape, \" Test:\", test.shape)\n",
        "\n",
        "    if \"release_date\" in train.columns:\n",
        "        train[\"release_date\"] = pd.to_datetime(train[\"release_date\"], errors=\"coerce\")\n",
        "        test[\"release_date\"] = pd.to_datetime(test[\"release_date\"], errors=\"coerce\")\n",
        "\n",
        "        train[\"year\"] = train[\"release_date\"].dt.year\n",
        "        train[\"month\"] = train[\"release_date\"].dt.month\n",
        "\n",
        "        test[\"year\"] = test[\"release_date\"].dt.year\n",
        "        test[\"month\"] = test[\"release_date\"].dt.month\n",
        "\n",
        "    for col in [\"year\", \"month\"]:\n",
        "        train[col] = pd.to_numeric(train.get(col, 2015), errors=\"coerce\").fillna(2015)\n",
        "        test[col] = pd.to_numeric(test.get(col, 2016), errors=\"coerce\").fillna(2016)\n",
        "\n",
        "    def parse_primary_genre(genres_cell):\n",
        "        if pd.isna(genres_cell):\n",
        "            return \"Unknown\"\n",
        "        try:\n",
        "            if isinstance(genres_cell, str):\n",
        "                try:\n",
        "                    val = json.loads(genres_cell)\n",
        "                except Exception:\n",
        "                    val = ast.literal_eval(genres_cell)\n",
        "            else:\n",
        "                val = genres_cell\n",
        "\n",
        "            if isinstance(val, list) and len(val) > 0:\n",
        "                first = val[0]\n",
        "                if isinstance(first, dict) and \"name\" in first:\n",
        "                    return first[\"name\"]\n",
        "                if isinstance(first, str):\n",
        "                    return first\n",
        "            if isinstance(val, dict) and \"name\" in val:\n",
        "                return val[\"name\"]\n",
        "            return str(val)\n",
        "        except Exception:\n",
        "            return \"Unknown\"\n",
        "\n",
        "    if \"primary_genre\" not in train.columns:\n",
        "        if \"genres\" in train.columns:\n",
        "            train[\"primary_genre\"] = train[\"genres\"].apply(parse_primary_genre)\n",
        "            if \"genres\" in test.columns:\n",
        "                test[\"primary_genre\"] = test[\"genres\"].apply(parse_primary_genre)\n",
        "            else:\n",
        "                test[\"primary_genre\"] = \"Unknown\"\n",
        "        else:\n",
        "            train[\"primary_genre\"] = \"Unknown\"\n",
        "            test[\"primary_genre\"] = \"Unknown\"\n",
        "\n",
        "    if \"label_roi3\" not in train.columns:\n",
        "        print(\"Creating label_roi3 from budget & revenue.\")\n",
        "        for df in [train, test]:\n",
        "            df[\"budget\"] = pd.to_numeric(df[\"budget\"], errors=\"coerce\")\n",
        "            df[\"revenue\"] = pd.to_numeric(df[\"revenue\"], errors=\"coerce\")\n",
        "            df[\"ROI\"] = (df[\"revenue\"] - df[\"budget\"]) / df[\"budget\"]\n",
        "            df[\"ROI\"] = df[\"ROI\"].replace([np.inf, -np.inf], np.nan)\n",
        "\n",
        "            def roi_to_class(roi):\n",
        "                if pd.isna(roi):\n",
        "                    return np.nan\n",
        "                if roi >= 2.5567:\n",
        "                    return \"Hit\"\n",
        "                if roi < 0.0049:\n",
        "                    return \"Flop\"\n",
        "                return \"Average\"\n",
        "\n",
        "            df[\"label_roi3\"] = df[\"ROI\"].apply(roi_to_class)\n",
        "\n",
        "    train = train.dropna(subset=[\"label_roi3\"])\n",
        "    test = test.dropna(subset=[\"label_roi3\"])\n",
        "\n",
        "    for col in [\"budget\", \"revenue\", \"year\", \"month\"]:\n",
        "        train[col] = pd.to_numeric(train[col], errors=\"coerce\")\n",
        "        test[col] = pd.to_numeric(test[col], errors=\"coerce\")\n",
        "\n",
        "    print(\"Finished preprocessing. Label distribution (train):\")\n",
        "    print(train[\"label_roi3\"].value_counts())\n",
        "\n",
        "    return train, test\n",
        "\n",
        "\n",
        "def train_models(train, test, random_state=42, use_optuna=False, n_optuna_trials=20):\n",
        "    \"\"\"\n",
        "    Train:\n",
        "    - Tang 2024 Optimized XGBoost (RandomizedSearchCV)  [baseline]\n",
        "    - Gupta-style Voting & Stacking Ensembles            [baseline]\n",
        "    - NEW: LightGBM                                      [novelty]\n",
        "    - NEW: CatBoost                                      [novelty]\n",
        "    - OPTIONAL NEW: Optuna-optimized XGBoost             [novelty]\n",
        "\n",
        "    Returns:\n",
        "        results: dict of model_name -> metrics dict\n",
        "    \"\"\"\n",
        "    num_features = [\"budget\", \"revenue\", \"year\", \"month\"]\n",
        "    cat_features = [\"primary_genre\"]\n",
        "    target = \"label_roi3\"\n",
        "\n",
        "    le = LabelEncoder()\n",
        "    y_train_enc = le.fit_transform(train[target])\n",
        "    y_test_enc = le.transform(test[target])\n",
        "\n",
        "    X_train = train[num_features + cat_features].copy()\n",
        "    X_test = test[num_features + cat_features].copy()\n",
        "\n",
        "    num_tf = Pipeline([(\"imp\", SimpleImputer(strategy=\"median\"))])\n",
        "    cat_tf = Pipeline(\n",
        "        [\n",
        "            (\"imp\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "            (\"oh\", OneHotEncoder(handle_unknown=\"ignore\")),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    pre = ColumnTransformer(\n",
        "        [\n",
        "            (\"num\", num_tf, num_features),\n",
        "            (\"cat\", cat_tf, cat_features),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    if xgb is not None:\n",
        "        print(\"\\n[Baseline] Tang 2024 Optimized XGBoost (RandomizedSearchCV)\")\n",
        "        clf_xgb = xgb.XGBClassifier(\n",
        "            objective=\"multi:softprob\",\n",
        "            num_class=len(le.classes_),\n",
        "            eval_metric=\"mlogloss\",\n",
        "            random_state=random_state,\n",
        "            n_estimators=300,\n",
        "        )\n",
        "\n",
        "        pipe_xgb = Pipeline([(\"pre\", pre), (\"xgb\", clf_xgb)])\n",
        "\n",
        "        param_dist = {\n",
        "            \"xgb__max_depth\": [3, 4, 5, 6],\n",
        "            \"xgb__learning_rate\": [0.02, 0.05, 0.1],\n",
        "            \"xgb__subsample\": [0.8, 1.0],\n",
        "            \"xgb__colsample_bytree\": [0.8, 1.0],\n",
        "            \"xgb__min_child_weight\": [1, 3, 5],\n",
        "        }\n",
        "\n",
        "        search = RandomizedSearchCV(\n",
        "            pipe_xgb,\n",
        "            param_distributions=param_dist,\n",
        "            n_iter=10,\n",
        "            scoring=\"f1_macro\",\n",
        "            cv=3,\n",
        "            n_jobs=-1,\n",
        "            verbose=1,\n",
        "            random_state=random_state,\n",
        "        )\n",
        "        search.fit(X_train, y_train_enc)\n",
        "        best_xgb = search.best_estimator_\n",
        "\n",
        "        pred_xgb_enc = best_xgb.predict(X_test)\n",
        "\n",
        "        results[\"Tang_XGBoost\"] = {\n",
        "            \"Accuracy\": accuracy_score(y_test_enc, pred_xgb_enc),\n",
        "            \"F1_macro\": f1_score(y_test_enc, pred_xgb_enc, average=\"macro\"),\n",
        "        }\n",
        "        print(\"Tang XGBoost Results:\", results[\"Tang_XGBoost\"])\n",
        "    else:\n",
        "        print(\"Skip XGBoost baseline (xgboost not installed).\")\n",
        "\n",
        "    print(\"\\n[Baseline] Gupta-style Ensemble Models (Voting & Stacking)\")\n",
        "    lr = LogisticRegression(max_iter=200)\n",
        "    svm = SVC(probability=True, kernel=\"rbf\", C=2.0, gamma=\"scale\", random_state=random_state)\n",
        "    rf = RandomForestClassifier(n_estimators=300, random_state=random_state)\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=300, learning_rate=0.05, random_state=random_state\n",
        "    )\n",
        "    knn = KNeighborsClassifier(n_neighbors=15)\n",
        "\n",
        "    voters = [(\"lr\", lr), (\"svm\", svm), (\"rf\", rf), (\"gb\", gb), (\"knn\", knn)]\n",
        "\n",
        "    soft_vote = VotingClassifier(estimators=voters, voting=\"soft\")\n",
        "    pipe_vote = Pipeline([(\"pre\", pre), (\"ens\", soft_vote)])\n",
        "    pipe_vote.fit(X_train, train[target])\n",
        "    pred_vote = pipe_vote.predict(X_test)\n",
        "    results[\"Voting\"] = {\n",
        "        \"Accuracy\": accuracy_score(test[target], pred_vote),\n",
        "        \"F1_macro\": f1_score(test[target], pred_vote, average=\"macro\"),\n",
        "    }\n",
        "    print(\"Voting Results:\", results[\"Voting\"])\n",
        "\n",
        "    stack = StackingClassifier(\n",
        "        estimators=voters,\n",
        "        final_estimator=LogisticRegression(max_iter=200),\n",
        "        stack_method=\"predict_proba\",\n",
        "    )\n",
        "    pipe_stack = Pipeline([(\"pre\", pre), (\"ens\", stack)])\n",
        "    pipe_stack.fit(X_train, train[target])\n",
        "    pred_stack = pipe_stack.predict(X_test)\n",
        "    results[\"Stacking\"] = {\n",
        "        \"Accuracy\": accuracy_score(test[target], pred_stack),\n",
        "        \"F1_macro\": f1_score(test[target], pred_stack, average=\"macro\"),\n",
        "    }\n",
        "    print(\"Stacking Results:\", results[\"Stacking\"])\n",
        "\n",
        "    if LGBMClassifier is not None:\n",
        "        print(\"\\n[Novelty] LightGBM Model\")\n",
        "        lgbm = LGBMClassifier(\n",
        "            n_estimators=500,\n",
        "            learning_rate=0.05,\n",
        "            max_depth=-1,\n",
        "            num_leaves=31,\n",
        "            subsample=0.8,\n",
        "            colsample_bytree=0.8,\n",
        "            random_state=random_state,\n",
        "        )\n",
        "        pipe_lgbm = Pipeline([(\"pre\", pre), (\"lgbm\", lgbm)])\n",
        "        pipe_lgbm.fit(X_train, y_train_enc)\n",
        "        pred_lgbm = pipe_lgbm.predict(X_test)\n",
        "        results[\"LightGBM\"] = {\n",
        "            \"Accuracy\": accuracy_score(y_test_enc, pred_lgbm),\n",
        "            \"F1_macro\": f1_score(y_test_enc, pred_lgbm, average=\"macro\"),\n",
        "        }\n",
        "        print(\"LightGBM Results:\", results[\"LightGBM\"])\n",
        "    else:\n",
        "        print(\"Skip LightGBM (lightgbm not installed).\")\n",
        "\n",
        "    if CatBoostClassifier is not None:\n",
        "        print(\"\\n[Novelty] CatBoost Model\")\n",
        "        cb = CatBoostClassifier(\n",
        "            depth=6,\n",
        "            learning_rate=0.05,\n",
        "            iterations=500,\n",
        "            loss_function=\"MultiClass\",\n",
        "            verbose=False,\n",
        "            random_state=random_state,\n",
        "        )\n",
        "        pipe_cb = Pipeline([(\"pre\", pre), (\"cb\", cb)])\n",
        "        pipe_cb.fit(X_train, y_train_enc)\n",
        "        pred_cb = pipe_cb.predict(X_test)\n",
        "        results[\"CatBoost\"] = {\n",
        "            \"Accuracy\": accuracy_score(y_test_enc, pred_cb),\n",
        "            \"F1_macro\": f1_score(y_test_enc, pred_cb, average=\"macro\"),\n",
        "        }\n",
        "        print(\"CatBoost Results:\", results[\"CatBoost\"])\n",
        "    else:\n",
        "        print(\"Skip CatBoost (catboost not installed).\")\n",
        "\n",
        "    if use_optuna and (optuna is not None) and (xgb is not None):\n",
        "        print(\"\\n[Novelty] Optuna-optimized XGBoost (this may take some time)...\")\n",
        "\n",
        "        def objective(trial):\n",
        "            params = {\n",
        "                \"max_depth\": trial.suggest_int(\"max_depth\", 3, 10),\n",
        "                \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 0.2),\n",
        "                \"subsample\": trial.suggest_float(\"subsample\", 0.5, 1.0),\n",
        "                \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.5, 1.0),\n",
        "                \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 10),\n",
        "                \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 500),\n",
        "            }\n",
        "\n",
        "            clf = xgb.XGBClassifier(\n",
        "                objective=\"multi:softprob\",\n",
        "                num_class=len(le.classes_),\n",
        "                eval_metric=\"mlogloss\",\n",
        "                random_state=random_state,\n",
        "                **params,\n",
        "            )\n",
        "            pipe = Pipeline([(\"pre\", pre), (\"xgb\", clf)])\n",
        "            scores = cross_val_score(\n",
        "                pipe,\n",
        "                X_train,\n",
        "                y_train_enc,\n",
        "                cv=3,\n",
        "                scoring=\"f1_macro\",\n",
        "                n_jobs=-1,\n",
        "            )\n",
        "            return scores.mean()\n",
        "\n",
        "        study = optuna.create_study(direction=\"maximize\")\n",
        "        study.optimize(objective, n_trials=n_optuna_trials)\n",
        "\n",
        "        best_params = study.best_params\n",
        "        print(\"Best Optuna XGBoost params:\", best_params)\n",
        "\n",
        "        clf_opt = xgb.XGBClassifier(\n",
        "            objective=\"multi:softprob\",\n",
        "            num_class=len(le.classes_),\n",
        "            eval_metric=\"mlogloss\",\n",
        "            random_state=random_state,\n",
        "            **best_params,\n",
        "        )\n",
        "        pipe_opt = Pipeline([(\"pre\", pre), (\"xgb\", clf_opt)])\n",
        "        pipe_opt.fit(X_train, y_train_enc)\n",
        "        pred_opt = pipe_opt.predict(X_test)\n",
        "        results[\"Optuna_XGBoost\"] = {\n",
        "            \"Accuracy\": accuracy_score(y_test_enc, pred_opt),\n",
        "            \"F1_macro\": f1_score(y_test_enc, pred_opt, average=\"macro\"),\n",
        "        }\n",
        "        print(\"Optuna_XGBoost Results:\", results[\"Optuna_XGBoost\"])\n",
        "    elif use_optuna:\n",
        "        print(\"Optuna XGBoost requested but optuna/xgboost not installed — skipping.\")\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "def evaluate_results(results_dict):\n",
        "    \"\"\"Print and compare all model results in a table.\"\"\"\n",
        "    print(\"\\n FINAL COMPARISON \")\n",
        "    df = pd.DataFrame(results_dict).T\n",
        "    if \"F1_macro\" in df.columns:\n",
        "        df = df.sort_values(\"F1_macro\", ascending=False)\n",
        "    print(df)\n",
        "    if \"F1_macro\" in df.columns:\n",
        "        print(\n",
        "            \"\\nBest model based on F1_macro:\",\n",
        "            df[\"F1_macro\"].idxmax(),\n",
        "            \"→ F1 =\",\n",
        "            round(df[\"F1_macro\"].max(), 4),\n",
        "        )\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    TRAIN_PATH = \"train_movies.csv\"\n",
        "    VAL_PATH = \"validation_movies.csv\"\n",
        "    TEST_PATH = \"test_movies.csv\"\n",
        "\n",
        "    train, test = load_and_preprocess(TRAIN_PATH, VAL_PATH, TEST_PATH)\n",
        "\n",
        "    results = train_models(train, test, random_state=42, use_optuna=False)\n",
        "    summary = evaluate_results(results)\n"
      ]
    }
  ]
}